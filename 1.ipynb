{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UBER FARE PREDICTION\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "df = pd.read_csv(\"uber.csv\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df.info()\n",
    "\n",
    "# Check for missing values\n",
    "df.isna().sum()\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "print(\"After dropping NaNs:\", df.shape)\n",
    "\n",
    "\n",
    "# Remove invalid fare amounts (negative or zero)\n",
    "df = df[df[\"fare_amount\"] > 0]\n",
    "\n",
    "# Remove invalid passenger counts\n",
    "df = df[(df[\"passenger_count\"] > 0) & (df[\"passenger_count\"] <= 6)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Boxplot before removing outliers\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.boxplot(x=df[\"fare_amount\"], color=\"lightcoral\")\n",
    "\n",
    "plt.title(\"Fare Amount Distribution (Before Outlier Removal)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Identify and remove outliers using IQR ---\n",
    "Q1 = df[\"fare_amount\"].quantile(0.25)\n",
    "Q3 = df[\"fare_amount\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Keep only data within 1.5 * IQR\n",
    "df = df[\n",
    "    (df[\"fare_amount\"] >= Q1 - 1.5 * IQR)\n",
    "    & (df[\"fare_amount\"] <= Q3 + 1.5 * IQR)\n",
    "]\n",
    "\n",
    "# Boxplot after removing outliers\n",
    "plt.figure(figsize=(6, 3))\n",
    "sns.boxplot(x=df[\"fare_amount\"], color=\"skyblue\")\n",
    "plt.title(\"Fare Amount Distribution (After Outlier Removal)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"Calculate great-circle distance (km) between two points.\"\"\"\n",
    "    R = 6371  # Earth radius (km)\n",
    "    lon1, lon2, lat1, lat2 = map(np.radians, [lon1, lon2, lat1, lat2])\n",
    "    dlon, dlat = lon2 - lon1, lat2 - lat1\n",
    "    a = np.sin(dlat / 2) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2) ** 2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "# Compute distance and add as new column\n",
    "df[\"distance_km\"] = haversine(\n",
    "    df[\"pickup_longitude\"],\n",
    "    df[\"pickup_latitude\"],\n",
    "    df[\"dropoff_longitude\"],\n",
    "    df[\"dropoff_latitude\"],\n",
    ")\n",
    "\n",
    "# Remove entries with zero or very small distance\n",
    "df = df[df[\"distance_km\"] > 0.1]\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "corr_matrix = df[[\"fare_amount\", \"distance_km\", \"passenger_count\"]].corr()\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Features and target\n",
    "X = df[[\"distance_km\", \"passenger_count\"]]\n",
    "y = df[\"fare_amount\"]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# ---- Linear Regression ----\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "\n",
    "# ---- Random Forest Regression ----\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return rmse, r2\n",
    "\n",
    "rmse_lr, r2_lr = evaluate_model(y_test, y_pred_lr, \"Linear Regression\")\n",
    "rmse_rf, r2_rf = evaluate_model(y_test, y_pred_rf, \"Random Forest\")\n",
    "\n",
    "comparison = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"Linear Regression\", \"Random Forest\"],\n",
    "        \"RMSE\": [rmse_lr, rmse_rf],\n",
    "        \"RÂ² Score\": [r2_lr, r2_rf],\n",
    "    }\n",
    ")\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(comparison)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33faf724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMAIL SPAM CLASSIFICATION\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"emails.csv\")\n",
    "df.head()\n",
    "df.info()\n",
    "print(\"Initial shape:\", df.shape)\n",
    "\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "print(\"After dropping missing values:\", df.shape)\n",
    "\n",
    "X = df.iloc[:, 1:-1]  # word count features\n",
    "y = df.iloc[:, -1]    # labels: 'spam' or 'not spam'\n",
    "\n",
    "# Check class distribution\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Class Distribution: Not Spam vs Spam\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Split data (stratify to preserve class ratio)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# KNN for multiple k values, store results\n",
    "k_values = [3, 5, 7]\n",
    "knn_results = {}  # store metrics for each k\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y_pred_knn = knn.predict(X_test_scaled)\n",
    "    \n",
    "    # Store metrics\n",
    "    knn_results[k] = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred_knn),\n",
    "        \"precision\": precision_score(y_test, y_pred_knn, pos_label=1),\n",
    "        \"recall\": recall_score(y_test, y_pred_knn, pos_label=1),\n",
    "        \"f1_score\": f1_score(y_test, y_pred_knn, pos_label=1),\n",
    "        \"confusion_matrix\": confusion_matrix(y_test, y_pred_knn)\n",
    "    }\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nKNN with k={k}\")\n",
    "    print(f\"Accuracy: {knn_results[k]['accuracy']:.3f}\")\n",
    "    print(f\"Precision: {knn_results[k]['precision']:.3f}\")\n",
    "    print(f\"Recall: {knn_results[k]['recall']:.3f}\")\n",
    "    print(f\"F1-score: {knn_results[k]['f1_score']:.3f}\")\n",
    "    print(f\"Confusion Matrix:\\n{knn_results[k]['confusion_matrix']}\")\n",
    "\n",
    "# Initialize SVM with default parameters\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Store metrics\n",
    "svm_metrics = {\n",
    "    \"accuracy\": accuracy_score(y_test, y_pred_svm),\n",
    "    \"precision\": precision_score(y_test, y_pred_svm, pos_label=1),\n",
    "    \"recall\": recall_score(y_test, y_pred_svm, pos_label=1),\n",
    "    \"f1_score\": f1_score(y_test, y_pred_svm, pos_label=1),\n",
    "    \"confusion_matrix\": confusion_matrix(y_test, y_pred_svm)\n",
    "}\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nSVM (Default Settings)\")\n",
    "print(f\"Accuracy: {svm_metrics['accuracy']:.3f}\")\n",
    "print(f\"Precision: {svm_metrics['precision']:.3f}\")\n",
    "print(f\"Recall: {svm_metrics['recall']:.3f}\")\n",
    "print(f\"F1-score: {svm_metrics['f1_score']:.3f}\")\n",
    "print(f\"Confusion Matrix:\\n{svm_metrics['confusion_matrix']}\")\n",
    "\n",
    "\n",
    "\n",
    "# Compare metrics for KNN (k=3) and SVM\n",
    "best_k = 3\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"]\n",
    "\n",
    "knn_scores = [\n",
    "    knn_results[best_k][\"accuracy\"],\n",
    "    knn_results[best_k][\"precision\"],\n",
    "    knn_results[best_k][\"recall\"],\n",
    "    knn_results[best_k][\"f1_score\"]\n",
    "]\n",
    "\n",
    "svm_scores = [\n",
    "    svm_metrics[\"accuracy\"],\n",
    "    svm_metrics[\"precision\"],\n",
    "    svm_metrics[\"recall\"],\n",
    "    svm_metrics[\"f1_score\"]\n",
    "]\n",
    "\n",
    "# Plot grouped bar chart\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(x - width/2, knn_scores, width, label=f'KNN (k={best_k})', color='blue')\n",
    "plt.bar(x + width/2, svm_scores, width, label='SVM', color='green')\n",
    "\n",
    "plt.xticks(x, metrics)\n",
    "plt.ylim(0,1.05)\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"KNN vs SVM: Classification Metrics\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df6f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUSTOMER BANK CHURN PREDICTION\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"Churn_Modelling.csv\")\n",
    "df.head()\n",
    "\n",
    "print(\"Initial shape:\", df.shape)\n",
    "\n",
    "df.info()\n",
    "\n",
    "# Drop irrelevant columns\n",
    "df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n",
    "\n",
    "# Encode categorical variables\n",
    "le_gender = LabelEncoder()\n",
    "df['Gender'] = le_gender.fit_transform(df['Gender'])\n",
    "\n",
    "le_geo = LabelEncoder()\n",
    "df['Geography'] = le_geo.fit_transform(df['Geography'])\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop('Exited', axis=1)\n",
    "y = df['Exited']\n",
    "\n",
    "# Train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "def build_and_train_nn(X_train, y_train, X_test, y_test, \n",
    "                       hidden_layers=[64,32], activation='relu', epochs=50):\n",
    "    # Initialize model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_layers[0], input_dim=X_train.shape[1], activation=activation))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for units in hidden_layers[1:]:\n",
    "        model.add(Dense(units, activation=activation))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(X_train, y_train, validation_split=0.2, epochs=epochs, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Activation: {activation}, Epochs: {epochs}\")\n",
    "    print(f\"Accuracy: {acc:.3f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Experiment with different activations and epochs\n",
    "activations = ['relu', 'tanh', 'sigmoid']\n",
    "epochs_list = [25, 50, 75]\n",
    "\n",
    "results = []\n",
    "\n",
    "for act in activations:\n",
    "    for ep in epochs_list:\n",
    "        model, history = build_and_train_nn(X_train_scaled, y_train, X_test_scaled, y_test,\n",
    "                                            activation=act, epochs=ep)\n",
    "        results.append((act, ep, model, history))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# One subplot per activation function\n",
    "activations = sorted(list(set([r[0] for r in results])))\n",
    "epochs_list = sorted(list(set([r[1] for r in results])))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for i, act in enumerate(activations):\n",
    "    plt.subplot(1, len(activations), i+1)\n",
    "    \n",
    "    for act_r, ep, model, history in results:\n",
    "        if act_r == act:\n",
    "            plt.plot(history.history['val_accuracy'], label=f'{ep} epochs')\n",
    "    \n",
    "    plt.title(f'Validation Accuracy - {act}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K NEAREST NEIGHBORS\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "df.head()\n",
    "\n",
    "df.shape\n",
    "df.info()\n",
    "\n",
    "# Check for missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "\n",
    "# Columns where zero is invalid and should be replaced\n",
    "zero_invalid_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "# Replace zeros with median values in these columns\n",
    "for col in zero_invalid_cols:\n",
    "    median_val = df[col].median()\n",
    "    df[col] = df[col].replace(0, median_val)\n",
    "\n",
    "# Split features and target\n",
    "X = df.drop('Outcome', axis=1)\n",
    "y = df['Outcome']\n",
    "\n",
    "# Train-test split (80-20)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "# Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Define parameters to experiment\n",
    "k_values = [3, 5, 7]\n",
    "distance_metrics = ['euclidean', 'manhattan', 'minkowski', 'chebyshev']\n",
    "minkowski_p = 3  # for Minkowski distance\n",
    "\n",
    "# Dictionary to store results\n",
    "knn_results = {}\n",
    "\n",
    "for metric in distance_metrics:\n",
    "    for k in k_values:\n",
    "        if metric == 'minkowski':\n",
    "            knn = KNeighborsClassifier(n_neighbors=k, metric=metric, p=minkowski_p)\n",
    "        else:\n",
    "            knn = KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "        \n",
    "        knn.fit(X_train_scaled, y_train)\n",
    "        y_pred = knn.predict(X_test_scaled)\n",
    "        \n",
    "        # Store metrics\n",
    "        knn_results[(metric, k)] = {\n",
    "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"error_rate\": 1 - accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred),\n",
    "            \"recall\": recall_score(y_test, y_pred),\n",
    "            \"f1_score\": f1_score(y_test, y_pred),\n",
    "            \"confusion_matrix\": confusion_matrix(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nKNN with k={k}, metric={metric}\")\n",
    "        print(f\"Accuracy: {knn_results[(metric, k)]['accuracy']:.3f}\")\n",
    "        print(f\"Error Rate: {knn_results[(metric, k)]['error_rate']:.3f}\")\n",
    "        print(f\"Precision: {knn_results[(metric, k)]['precision']:.3f}\")\n",
    "        print(f\"Recall: {knn_results[(metric, k)]['recall']:.3f}\")\n",
    "        print(f\"F1-score: {knn_results[(metric, k)]['f1_score']:.3f}\")\n",
    "        print(\"Confusion Matrix:\\n\", knn_results[(metric, k)]['confusion_matrix'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert results to DataFrame for plotting\n",
    "results_df = pd.DataFrame(knn_results).T  # transpose so (metric, k) is index\n",
    "results_df[['accuracy', 'precision', 'recall', 'f1_score']].plot(\n",
    "    kind='bar', figsize=(14,6)\n",
    ")\n",
    "plt.title(\"KNN Performance Metrics for Different k and Distance Metrics\")\n",
    "plt.xlabel(\"(Distance Metric, k)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcc178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Clustering K MEANS AND HIERARCHICAL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = pd.read_csv(\"sales_data_sample.csv\", encoding='latin1')\n",
    "df.head()\n",
    "df.shape\n",
    "df.info()\n",
    "\n",
    "\n",
    "#Check for missing values\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Numeric features selected for clustering\n",
    "numeric_cols = ['QUANTITYORDERED', 'PRICEEACH', 'SALES']\n",
    "\n",
    "# Drop rows with missing values in these columns\n",
    "X = df[numeric_cols].dropna()\n",
    "\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Shape of scaled data:\", X_scaled.shape)\n",
    "\n",
    "# Fill missing values with median\n",
    "df_numeric = df_numeric.fillna(df_numeric.median())\n",
    "\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_numeric)\n",
    "X_scaled.shape\n",
    "\n",
    "\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(pd.DataFrame(X_scaled, columns=numeric_cols).corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap of Numeric Features\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Scatter plot example: SALES vs PRICEEACH\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x=X['PRICEEACH'], y=X['SALES'])\n",
    "plt.title(\"SALES vs PRICEEACH\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Elbow Method\n",
    "inertia = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot Elbow Method\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(k_range, inertia, 'bo-')\n",
    "plt.xlabel(\"Number of clusters (k)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.show()\n",
    "\n",
    "# Choose optimal k (k=4 from elbow plot)\n",
    "k_optimal = 4\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)\n",
    "X['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Reduce to 2D using PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=X['cluster'], cmap='viridis', marker='o')\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.title(f\"K-Means Clusters Visualization (k={k_optimal})\")\n",
    "plt.show()\n",
    "\n",
    "# Choose optimal k (k=3 from elbow plot)\n",
    "k_optimal = 3\n",
    "kmeans = KMeans(n_clusters=k_optimal, random_state=42, n_init=10)\n",
    "X['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Reduce to 2D using PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], c=X['cluster'], cmap='viridis', marker='o')\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.title(f\"K-Means Clusters Visualization (k={k_optimal})\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute linkage matrix\n",
    "Z = linkage(X_scaled, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(10,6))\n",
    "dendrogram(Z, truncate_mode='level', p=5)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram (truncated)\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n",
    "\n",
    "# Form flat clusters (example: 4 clusters)\n",
    "clusters_h = fcluster(Z, t=4, criterion='maxclust')\n",
    "X['cluster_hier'] = clusters_h\n",
    "\n",
    "\n",
    "# Combine cluster labels with original numeric features\n",
    "X_summary = X.copy()\n",
    "X_summary[numeric_cols] = df[numeric_cols].loc[X.index]\n",
    "\n",
    "# Summary statistics per cluster\n",
    "cluster_stats = X_summary.groupby('cluster')[numeric_cols].mean()\n",
    "display(cluster_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
